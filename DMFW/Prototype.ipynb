{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/anaconda3/lib/python3.7/site-packages (1.5.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:04:59.966556Z",
     "start_time": "2021-08-02T17:04:59.960151Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import Graphs\n",
    "import networkx as nx\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.optimizer import required\n",
    "from torchinfo import summary\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:01.290078Z",
     "start_time": "2021-08-02T17:05:01.287232Z"
    }
   },
   "outputs": [],
   "source": [
    "bkdatafolder = \"../BKDataCleaned/\"\n",
    "dataset_name = os.listdir(bkdatafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:04.922052Z",
     "start_time": "2021-08-02T17:05:04.897192Z"
    },
    "code_folding": [
     68
    ]
   },
   "outputs": [],
   "source": [
    "def createDictFloor(floor_name):\n",
    "    getdict = {}\n",
    "    floors = [floor for floor in dataset_name if floor_name in floor]\n",
    "    for name in floors:\n",
    "        floorname = name.split(\".\")[0]\n",
    "        getdict[floorname] = pd.read_csv(bkdatafolder+name, index_col=0, parse_dates=[\"Date\"])\n",
    "        getdict[floorname] = getdict[floorname].sort_index()\n",
    "    return getdict\n",
    "\n",
    "def Missing_values(data):\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total,percent], axis=1, keys=['Total', 'Pourcentage'])\n",
    "    print (missing_data[(percent>0)],'\\n' )\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "def createPlot(date1, date2, features, data):\n",
    "    for floor in data.keys():\n",
    "        data[floor].resample(\"5T\").mean()[features][date1:date2].plot(figsize=(20,7))\n",
    "        \n",
    "\n",
    "def createDTFeat(date1, date2, datadict, features, resample_method=\"sum\" ,scale=True):\n",
    "    resample_move = {}\n",
    "    index_nan = {}\n",
    "    index_small = {}\n",
    "    floors = list(datadict.keys())\n",
    "    dates = []\n",
    "    scalers = {}\n",
    "    for data in datadict.keys():\n",
    "        if resample_method == \"sum\":\n",
    "            resample_move[data] = floor_dict[data][date1:date2].resample(\n",
    "                \"5min\").sum()\n",
    "            \n",
    "        elif resample_method == \"max\":\n",
    "            resample_move[data] = floor_dict[data][date1:date2].resample(\n",
    "                \"5min\").max().bfill()\n",
    "        elif resample_method == \"mean\":\n",
    "            resample_move[data] = floor_dict[data][date1:date2].resample(\n",
    "                \"5min\").mean().bfill()\n",
    "            \n",
    "        cols = resample_move[data].columns\n",
    "        idx = resample_move[data].index\n",
    "\n",
    "        if scale:\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(resample_move[data])\n",
    "            resample_move[data] = pd.DataFrame(scaler.transform(\n",
    "                resample_move[data]),\n",
    "                                               columns=cols,\n",
    "                                               index=idx)\n",
    "            scalers[data] = scaler\n",
    "\n",
    "        resample_move[data][\"weekday\"] = resample_move[data].index.day_name()\n",
    "        resample_move[data][\"date\"] = resample_move[data].index.date\n",
    "        resample_move[data][\"time\"] = resample_move[data].index.time\n",
    "\n",
    "        nan = np.where(pd.isnull(resample_move[data][features]))[0]\n",
    "        index_nan[data] = np.unique(resample_move[data].iloc[nan][\"time\"])\n",
    "\n",
    "    return resample_move, scalers, index_nan\n",
    "\n",
    "def getInfoTimeShape(datadict):\n",
    "    for floor in datadict.keys():\n",
    "        data = datadict[floor]\n",
    "        print(\"Floor : {} , shape :{} , TimeMin {} , TimeMax {}\".format(floor,data.shape, data.index.min(), data.index.max()))\n",
    "        Missing_values(data)\n",
    "        \n",
    "def cleanNan(data,idx_nan):\n",
    "    index=[]\n",
    "    for k,v in idx_nan.items():\n",
    "        for ele in v:\n",
    "            index.append(ele)\n",
    "    mynan = set(index)\n",
    "    newdata = data.copy()\n",
    "    remain_date = []\n",
    "    for floor in idx_nan.keys():\n",
    "        datafloor = data[floor]\n",
    "        todropnan = datafloor[datafloor[\"date\"].isin(list(mynan))].index\n",
    "        datafloor = datafloor.drop(todropnan)\n",
    "        newdata[floor] = datafloor\n",
    "        for date in datafloor[\"date\"]:\n",
    "            remain_date.append(str(date))\n",
    "    remain_date = sorted(set(remain_date))\n",
    "    return data, remain_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:07.577340Z",
     "start_time": "2021-08-02T17:05:06.548512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floor7Z4 Start: 2019-03-06 14:46:00 End: 2019-12-31 23:59:00 Count:416243\n",
      "\n",
      "Floor7Z5 Start: 2019-03-06 14:55:00 End: 2019-12-31 23:59:00 Count:407698\n",
      "\n",
      "Floor7Z2 Start: 2019-03-06 14:48:00 End: 2019-12-31 23:59:00 Count:424701\n",
      "\n",
      "Floor7Z1 Start: 2019-03-06 14:50:00 End: 2019-12-31 23:59:00 Count:424099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date1 = \"2019-03-07\"\n",
    "date2 = \"2019-12-31\"\n",
    "floor_dict = createDictFloor(\"Floor7\")\n",
    "for data in floor_dict.keys():\n",
    "    zone = floor_dict[data]\n",
    "    print(\"{} Start: {} End: {} Count:{}\".format(data,zone.index.min(),zone.index.max(), zone.shape[0]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:08.632869Z",
     "start_time": "2021-08-02T17:05:08.186087Z"
    }
   },
   "outputs": [],
   "source": [
    "feature = \"temperature\"\n",
    "resample_method = \"max\"\n",
    "resample,scalers, index_nan = createDTFeat(date1, date2, floor_dict, feature,resample_method=resample_method)\n",
    "cleanedData, remain_date = cleanNan(resample, index_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:09.164487Z",
     "start_time": "2021-08-02T17:05:09.042632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floor : Floor7Z4 , shape :(86400, 9) , TimeMin 2019-03-07 00:00:00 , TimeMax 2019-12-31 23:55:00\n",
      "Empty DataFrame\n",
      "Columns: [Total, Pourcentage]\n",
      "Index: [] \n",
      "\n",
      "Floor : Floor7Z5 , shape :(86400, 9) , TimeMin 2019-03-07 00:00:00 , TimeMax 2019-12-31 23:55:00\n",
      "Empty DataFrame\n",
      "Columns: [Total, Pourcentage]\n",
      "Index: [] \n",
      "\n",
      "Floor : Floor7Z2 , shape :(86400, 9) , TimeMin 2019-03-07 00:00:00 , TimeMax 2019-12-31 23:55:00\n",
      "Empty DataFrame\n",
      "Columns: [Total, Pourcentage]\n",
      "Index: [] \n",
      "\n",
      "Floor : Floor7Z1 , shape :(86400, 9) , TimeMin 2019-03-07 00:00:00 , TimeMax 2019-12-31 23:55:00\n",
      "Empty DataFrame\n",
      "Columns: [Total, Pourcentage]\n",
      "Index: [] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "getInfoTimeShape(cleanedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:11.165442Z",
     "start_time": "2021-08-02T17:05:11.150387Z"
    }
   },
   "outputs": [],
   "source": [
    "def rolling_window(series, window_size):\n",
    "    return np.array([series[i : (i + window_size)] for i in range(0, series.shape[0] - window_size + 1)])\n",
    "\n",
    "def to_timeseries_input(series, lookback, predictions):\n",
    "    inputs = rolling_window(series[:-predictions], lookback)\n",
    "    outputs = rolling_window(series[lookback:], predictions)\n",
    "    return inputs, outputs\n",
    "\n",
    "def createDataByDate(datadict, features, dates):\n",
    "    databyDate = defaultdict(lambda : defaultdict(dict))\n",
    "    for date in dates:\n",
    "        for floor in datadict.keys():\n",
    "            databyDate[date][floor] = np.asarray(datadict[floor][date][features])\n",
    "    return databyDate\n",
    "\n",
    "def splitDate(dates,cutoff):\n",
    "    train_date = [x for x in dates if x <= cutoff]\n",
    "    test_date = sorted(list(set(dates) - set(train_date)))\n",
    "    return train_date, test_date\n",
    "\n",
    "def getInfoDataByDate(data, dates):\n",
    "    for date in dates:\n",
    "        print(\"Date : {}\".format(date))\n",
    "        for floor in data[date]:\n",
    "            print(\"{} shape : {}\".format(floor,data[date][floor].shape))\n",
    "            \n",
    "def LoaderByZone(data,zone_name,dates, lookback, lookahead, batch_size, shuffle=False):\n",
    "    loaderZ = {}\n",
    "    for i,date in enumerate(dates):\n",
    "        haruharu = data[date][zone_name]\n",
    "        trainx, trainy = to_timeseries_input(haruharu, lookback, lookahead,)\n",
    "        xshape = torch.tensor(trainx, dtype=torch.float).unsqueeze(-1)\n",
    "        yshape = torch.tensor(trainy, dtype=torch.float)\n",
    "        tensorwrap = TensorDataset(xshape,yshape)\n",
    "        loaderxy = DataLoader(tensorwrap,batch_size = batch_size, shuffle=shuffle, drop_last=True)\n",
    "        loaderZ[date] = loaderxy\n",
    "    return loaderZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:11.728478Z",
     "start_time": "2021-08-02T17:05:11.623899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floor7Z4 Start: 2019-03-07 00:00:00 End: 2019-12-31 23:55:00 Count:86400\n",
      "Floor7Z4 Dates: 300\n",
      "Floor7Z5 Start: 2019-03-07 00:00:00 End: 2019-12-31 23:55:00 Count:86400\n",
      "Floor7Z5 Dates: 300\n",
      "Floor7Z2 Start: 2019-03-07 00:00:00 End: 2019-12-31 23:55:00 Count:86400\n",
      "Floor7Z2 Dates: 300\n",
      "Floor7Z1 Start: 2019-03-07 00:00:00 End: 2019-12-31 23:55:00 Count:86400\n",
      "Floor7Z1 Dates: 300\n"
     ]
    }
   ],
   "source": [
    "for data in cleanedData.keys():\n",
    "    zone = cleanedData[data]\n",
    "    print(\"{} Start: {} End: {} Count:{}\".format(data,zone.index.min(),zone.index.max(), zone.shape[0]))\n",
    "    print(\"{} Dates: {}\".format(data,len(np.unique(zone[\"date\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:12.397136Z",
     "start_time": "2021-08-02T17:05:12.124665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date : 2019-03-07\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-08\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-09\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-10\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-11\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-12\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-13\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-14\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-15\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-16\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-17\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-18\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-19\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-20\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-21\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-22\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-23\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-24\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-25\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-26\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-27\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-28\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-29\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-30\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-03-31\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-01\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-02\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-03\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-04\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-05\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-06\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-07\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-08\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-09\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-10\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-11\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-12\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-13\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-14\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-15\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-16\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-17\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-18\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-19\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-20\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-21\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-22\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-23\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-24\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-25\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-26\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-27\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-28\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-29\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-04-30\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-05-01\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-05-02\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-05-03\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-05-04\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-05-05\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-05-06\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-05-07\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n",
      "Date : 2019-05-08\n",
      "Floor7Z4 shape : (288,)\n",
      "Floor7Z5 shape : (288,)\n",
      "Floor7Z2 shape : (288,)\n",
      "Floor7Z1 shape : (288,)\n"
     ]
    }
   ],
   "source": [
    "cutting_date = \"2019-05-08\"\n",
    "train_date, test_date = splitDate(remain_date, cutting_date)\n",
    "databyDate = createDataByDate(cleanedData, feature, remain_date)\n",
    "getInfoDataByDate(databyDate, train_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:05:15.111516Z",
     "start_time": "2021-08-02T17:05:15.108442Z"
    }
   },
   "outputs": [],
   "source": [
    "def getRatio(ratio, lookbackinit):\n",
    "    lookahead = int(ratio*lookbackinit)\n",
    "    return lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T21:08:22.071325Z",
     "start_time": "2021-08-02T21:08:21.813323Z"
    }
   },
   "outputs": [],
   "source": [
    "z1 = \"Floor7Z1\"\n",
    "z2 = \"Floor7Z2\"\n",
    "z4 = \"Floor7Z4\"\n",
    "z5 = \"Floor7Z5\"\n",
    "\n",
    "lookback = 13\n",
    "lookahead = 1\n",
    "batch_size = 32\n",
    "loaderZ1train = LoaderByZone(databyDate, z1, train_date, lookback, lookahead, batch_size, shuffle=True)\n",
    "loaderZ1test = LoaderByZone(databyDate, z1, test_date, lookback, lookahead, batch_size)\n",
    "\n",
    "loaderZ2train = LoaderByZone(databyDate, z2, train_date, lookback, lookahead, batch_size, shuffle=True)\n",
    "loaderZ2test = LoaderByZone(databyDate, z2, test_date, lookback, lookahead, batch_size)\n",
    "\n",
    "loaderZ4train = LoaderByZone(databyDate, z4, train_date, lookback, lookahead, batch_size, shuffle=True)\n",
    "loaderZ4test = LoaderByZone(databyDate, z4, test_date, lookback, lookahead, batch_size)\n",
    "\n",
    "loaderZ5train = LoaderByZone(databyDate, z5, train_date, lookback, lookahead, batch_size, shuffle=True)\n",
    "loaderZ5test = LoaderByZone(databyDate, z5, test_date, lookback, lookahead, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T21:08:22.450354Z",
     "start_time": "2021-08-02T21:08:22.446211Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = [loaderZ1train, loaderZ2train, loaderZ4train, loaderZ5train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T21:08:22.817679Z",
     "start_time": "2021-08-02T21:08:22.811927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_iterations = len(train_date)*len(loaderZ2train[\"2019-03-08\"])\n",
    "nb_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:23:15.454263Z",
     "start_time": "2021-08-02T17:23:15.435491Z"
    },
    "code_folding": [
     0,
     14
    ]
   },
   "outputs": [],
   "source": [
    "def proj_2shape(x,s=8):\n",
    "    shape = x.shape\n",
    "    if torch.linalg.norm(x,ord=1)==s and torch.all(x>0):\n",
    "        return x\n",
    "    u,_ = torch.sort(torch.abs(x),dim=0,descending=True)\n",
    "    cumsum = torch.cumsum(u, dim=0)\n",
    "    arange = torch.arange(1, shape[0]+1)\n",
    "    rep_arange = arange.unsqueeze(1).repeat(1,shape[1])\n",
    "    rho = torch.count_nonzero((u*rep_arange > (cumsum - s)), dim=0)\n",
    "    theta = (cumsum[rho-1, torch.arange(0,shape[1])] - s)/rho\n",
    "    proj = (torch.abs(x)-theta).clamp(min=0)\n",
    "    proj*= torch.sign(x)\n",
    "    return proj\n",
    "\n",
    "def proj_l1(x, s=8):\n",
    "    shape = x.shape\n",
    "    if len(shape) == 4:\n",
    "        proj = torch.zeros_like(x)\n",
    "        for first_dim in range(x.shape[0]):\n",
    "            for second_dim in range(x.shape[1]):\n",
    "                inner_tensor = x[first_dim][second_dim]\n",
    "                inner_proj = proj_2shape(inner_tensor,s=s)\n",
    "                proj[first_dim][second_dim] = inner_proj\n",
    "                \n",
    "    elif len(shape) == 3:\n",
    "        proj = torch.zeros_like(x)\n",
    "        for first_dim in range(x.shape[0]):\n",
    "            inner_tensor = x[first_dim]\n",
    "            inner_proj = proj_2shape(inner_tensor,s=s)\n",
    "            proj[first_dim] = inner_proj\n",
    "        \n",
    "    elif len(shape) == 2:\n",
    "        proj = proj_2shape(x,s=s)\n",
    "        \n",
    "    elif len(shape) == 1:\n",
    "        u,_ = torch.sort(torch.abs(x),descending=True)\n",
    "        cumsum = torch.cumsum(u,dim=0)\n",
    "        arange = torch.arange(1,shape[0]+1)\n",
    "        rho = torch.count_nonzero((u*arange > (cumsum - s)))\n",
    "        theta = (cumsum[rho-1] - s)/rho\n",
    "        proj = (torch.abs(x)-theta).clamp(min=0)\n",
    "        proj*= torch.sign(x)\n",
    "    return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T10:44:42.183924Z",
     "start_time": "2021-07-31T10:44:42.170071Z"
    },
    "code_folding": [
     0,
     16
    ]
   },
   "outputs": [],
   "source": [
    "# #def simpleLMO(x,radius,p=1):\n",
    "#     tolerance = 1e-10\n",
    "#     if p == 1:\n",
    "#         v = torch.zeros_like(x)\n",
    "#         maxIdx = torch.argmax(torch.abs(x))\n",
    "#         v.view(-1)[maxIdx] = -radius * torch.sign(x.view(-1)[maxIdx])\n",
    "#         return v\n",
    "#     elif p == 2:\n",
    "#         x_norm = float(torch.norm(x, p=2))\n",
    "#         if x_norm > tolerance:\n",
    "#             return -radius * x.div(x_norm)\n",
    "#         else:\n",
    "#             return torch.zeros_like(x)\n",
    "#     elif p == float('inf'):\n",
    "#         return torch.full_like(x, fill_value=radius).masked_fill_(x > 0, -radius)\n",
    "\n",
    "# #def simpleProject(x,radius,p=1):\n",
    "#     if p==1:\n",
    "#         x_norm = torch.norm(x, p=1)\n",
    "#         if x_norm > radius:\n",
    "#             sorted_ = torch.sort(torch.abs(x.flatten()), descending=True).values\n",
    "#             running_mean = (torch.cumsum(sorted_, 0) - radius)/torch.arange(1,sorted_.numel() + 1)                                                                                  \n",
    "#             is_less_or_equal = sorted_ <= running_mean\n",
    "#             idx = is_less_or_equal.numel() - is_less_or_equal.sum() - 1\n",
    "#             return torch.sign(x) * torch.max(torch.abs(x) - running_mean[idx], torch.zeros_like(x))\n",
    "#         else:\n",
    "#             return x\n",
    "#     elif p == 2:\n",
    "#         x_norm = torch.norm(x, p=2)\n",
    "#         return radius * x.div(x_norm) if x_norm > radius else x\n",
    "#     elif p == float('inf'):\n",
    "#         return torch.clamp(x, min=-radius, max=radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:23:16.913373Z",
     "start_time": "2021-08-02T17:23:16.899350Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def lmo(x,radius):\n",
    "    \"\"\"Returns v with norm(v, self.p) <= r minimizing v*x\"\"\"\n",
    "    shape = x.shape\n",
    "    if len(shape) == 4:\n",
    "        v = torch.zeros_like(x)\n",
    "        for first_dim in range(shape[0]):\n",
    "            for second_dim in range(shape[1]):\n",
    "                inner_x = x[first_dim][second_dim]\n",
    "                rows, cols = x[first_dim][second_dim].shape\n",
    "                v[first_dim][second_dim] = torch.zeros_like(inner_x)\n",
    "                maxIdx = torch.argmax(torch.abs(inner_x),0)\n",
    "                for col in range(cols):\n",
    "                    v[first_dim][second_dim][maxIdx[col],col] = -radius*torch.sign(inner_x[maxIdx[col],col])\n",
    "    elif len(shape) == 3:\n",
    "        v = torch.zeros_like(x)\n",
    "        for first_dim in range(shape[0]):\n",
    "            inner_x = x[first_dim]\n",
    "            rows, cols = x[first_dim].shape\n",
    "            v[first_dim] = torch.zeros_like(inner_x)\n",
    "            maxIdx = torch.argmax(torch.abs(inner_x),0)\n",
    "            for col in range(cols):\n",
    "                v[first_dim][maxIdx[col],col] = -radius*torch.sign(inner_x[maxIdx[col],col])\n",
    "                    \n",
    "    elif len(shape)==2:\n",
    "        rows, cols = x.shape\n",
    "        v = torch.zeros_like(x)\n",
    "        maxIdx = torch.argmax(torch.abs(x),0)\n",
    "        for col in range(cols):\n",
    "            v[maxIdx[col],col] = -radius*torch.sign(x[maxIdx[col],col])\n",
    "                \n",
    "    else : \n",
    "        v = torch.zeros_like(x)\n",
    "        maxIdx = torch.argmax(torch.abs(x))\n",
    "        v.view(-1)[maxIdx] = -radius * torch.sign(x.view(-1)[maxIdx])\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T17:23:35.943816Z",
     "start_time": "2021-08-02T17:23:35.913974Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class DMFW(optim.Optimizer):\n",
    "    def __init__(self, params, eta_coef=required, eta_exp=required, L=required, matrix_line=required, reg_coef=required,radius=required):\n",
    "        if eta_coef is not required and eta_coef <=0.:\n",
    "            raise ValueError(\"Invalid eta : {}\".format(eta_coef))\n",
    "        if eta_exp is not required and (eta_exp == 0.5):\n",
    "            raise ValueError(\"Invalid eta_exp : {}\".format(eta_exp))\n",
    "        defaults = dict(eta_coef=eta_coef, eta_exp=eta_exp,L=L, matrix_line=matrix_line,reg_coef = reg_coef,radius=radius)\n",
    "        super(DMFW,self).__init__(params,defaults)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            self.eta_coef = group[\"eta_coef\"]\n",
    "            self.eta_exp = group[\"eta_exp\"]\n",
    "            self.reg_coef = group[\"reg_coef\"]\n",
    "            self.A = group[\"matrix_line\"]\n",
    "            self.L = group[\"L\"]\n",
    "            self.radius = group[\"radius\"]\n",
    "        self.num_layers = len(self.param_groups[0]['params'])\n",
    "        self.dim = [k.shape for k in self.param_groups[0]['params']]\n",
    "        self.G = [[torch.rand(k) for k in self.dim] for l in range(self.L)]\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def initValue(self,closure):\n",
    "        self.w_dict = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            if closure is not None:\n",
    "                with torch.enable_grad():\n",
    "                    closure()\n",
    "            for k,weight in enumerate(group[\"params\"]):\n",
    "                if weight.grad is None:\n",
    "                    raise ValueError(\"Gradient is None\")\n",
    "                self.w_dict[k][\"g\"] = weight.grad.detach().clone()\n",
    "                self.w_dict[k][\"w\"] = weight.detach().clone()\n",
    "                \n",
    "    def neighborsAverage(self, neighbors):\n",
    "        for group in self.param_groups:\n",
    "            for k,weight in enumerate(group[\"params\"]):\n",
    "                weighted_tmp = torch.zeros(self.dim[k])\n",
    "                weighted_grad_tmp = torch.zeros(self.dim[k])\n",
    "                for j in range(len(neighbors)):\n",
    "                    weighted_tmp += self.A[j]*neighbors[j].w_dict[k][\"w\"]\n",
    "                    weighted_grad_tmp += self.A[j]*neighbors[j].w_dict[k][\"g\"]\n",
    "                self.w_dict[k][\"y\"] = weighted_tmp\n",
    "                self.w_dict[k][\"ds\"] = weighted_grad_tmp\n",
    "                \n",
    "\n",
    "    def step(self, l, closure):\n",
    "        if l == 0:\n",
    "            self.init_gap = 0\n",
    "        eta = min(self.eta_coef/(l+1)**self.eta_exp, 1)\n",
    "        for group in self.param_groups:\n",
    "            if closure is not None:\n",
    "                with torch.enable_grad():\n",
    "                    closure()\n",
    "            self.gap = 0\n",
    "            for k,weight in enumerate(group[\"params\"]):\n",
    "                a = lmo(weight.grad.data,self.radius)\n",
    "                self.gap += torch.sum(torch.mul(weight.grad.data, weight.data - a))\n",
    "                v = proj_l1(self.G[l][k], s=self.radius)\n",
    "                #v = lmo(self.G[l][k] - 0.5 + torch.rand_like(self.G[l][k]), s= self.radius)\n",
    "                if weight.grad is None:\n",
    "                    raise ValueError(\"Grad is None\")\n",
    "                self.w_dict[k][\"grad_old\"] = weight.grad.detach().clone()\n",
    "                weight.data = self.w_dict[k]['y']*(1-eta) + eta*v\n",
    "                #print(\"weight {}\".format(torch.linalg.norm(weight.data,ord=1,dim=0)))\n",
    "                self.w_dict[k][\"w\"] = weight.detach().clone()\n",
    "                \n",
    "            self.init_gap += self.gap\n",
    "            self.init_gap /= (l+1)\n",
    "            \n",
    "            \n",
    "            with torch.enable_grad():\n",
    "                closure()\n",
    "            for k,weight in enumerate(group[\"params\"]):\n",
    "                self.G[l][k] -= 0.5*self.w_dict[k][\"ds\"]*self.reg_coef\n",
    "                #self.G[l][k] += self.w_dict[k][\"ds\"]*self.reg_coef\n",
    "                if weight.grad is None :\n",
    "                    raise ValueError(\"Grad is none\")\n",
    "                weight.grad.add_(-self.w_dict[k][\"grad_old\"])\n",
    "                self.w_dict[k][\"g\"] = weight.grad.detach().clone() + self.w_dict[k][\"ds\"]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:19:37.845343Z",
     "start_time": "2021-08-02T22:19:37.806817Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, graph, loaders, model, model_param, loss,\n",
    "                 num_iterations):\n",
    "        self.graph = graph\n",
    "        self.num_nodes = graph.number_of_nodes()\n",
    "        self.model = model\n",
    "        self.param = model_param\n",
    "        self.A = torch.tensor(nx.adjacency_matrix(graph).toarray())\n",
    "        self.dataloader = loaders\n",
    "        self.num_iterations = num_iterations\n",
    "        self.loss = loss\n",
    "        self.obj_values = np.ndarray((self.num_iterations + 1, 4),\n",
    "                                     dtype='float')\n",
    "\n",
    "        self.optimizers = [0.] * self.num_nodes\n",
    "        self.models = [0.] * self.num_nodes\n",
    "        self.losses = [0.] * self.num_nodes\n",
    "        #self.gaps = [0.]*self.num_nodes\n",
    "\n",
    "    def reset(self):\n",
    "        self.optimizers = [0.] * self.num_nodes\n",
    "        self.models = [0.] * self.num_nodes\n",
    "        self.losses = [0.] * self.num_nodes\n",
    "        self.gaps = [0.] * self.num_nodes\n",
    "\n",
    "        self.obj_values = np.ndarray((self.num_iterations + 1, 4),\n",
    "                                     dtype='float')\n",
    "        \n",
    "    def __nodeInit(self, data, label):\n",
    "        nodewrap = TensorDataset(data, label)\n",
    "        nodes = DataLoader(nodewrap, batch_size=data.size(0), shuffle=False)\n",
    "        return nodes\n",
    "\n",
    "    def weight_reset(self, layer):\n",
    "        if isinstance(layer, nn.BatchNorm1d) or isinstance(layer, nn.Linear) or isinstance(layer,nn.Conv1d):\n",
    "            layer.reset_parameters()\n",
    "            \n",
    "    def initModelWeight(self, model):\n",
    "        for name,param in model.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.constant_(param,0.)\n",
    "            elif 'weight' in name:\n",
    "                if not 'batch' in name:\n",
    "                    nn.init.xavier_normal_(param)\n",
    "                else:\n",
    "                    nn.init.uniform_(param)\n",
    "\n",
    "    def saveCheckPts(self, t, path):\n",
    "        check_pts = {}\n",
    "        for i in range(self.num_nodes):\n",
    "            ckp_i = {\n",
    "                \"t\": t,\n",
    "                \"weight\": [param for param in self.models[i].parameters()],\n",
    "                \"optimizer_weight\": self.optimizers[i].w_dict,\n",
    "                \"oracles\": self.optimizers[i].G,\n",
    "                \"loss\": self.losses[i]\n",
    "            }  #,\n",
    "            #\"avg_loss\": self.avg_loss[i]}\n",
    "            check_pts[i] = ckp_i\n",
    "        torch.save(check_pts, path + \"checkpts_models\" + \"_\" + str(t) + \".tar\")\n",
    "        \n",
    "    def plotPrediction(self, true, pred,date,path_to_save):\n",
    "        fig = plt.figure(figsize=(5,3))\n",
    "        plt.suptitle(\"{}\".format(date))\n",
    "        plt.plot(true)\n",
    "        plt.plot(pred)\n",
    "        #plt.show()\n",
    "        fig.savefig(os.path.join(path_to_save,date))\n",
    "        plt.close()\n",
    "\n",
    "    def train(self, optimizer, L, eta_coef, eta_exp, reg_coef, radius, path_figure_date):\n",
    "        seed_everything()\n",
    "        self.reset()\n",
    "        \n",
    "        z1, z2, z4, z5 = self.dataloader\n",
    "\n",
    "        for i in range(self.num_nodes):\n",
    "            self.models[i] = self.model(*self.param)\n",
    "            self.optimizers[i] = optimizer(self.models[i].parameters(),\n",
    "                                           eta_coef=eta_coef,\n",
    "                                           eta_exp=eta_exp,\n",
    "                                           L=L,\n",
    "                                           matrix_line=self.A[i],\n",
    "                                           reg_coef=reg_coef,\n",
    "                                           radius=radius)\n",
    "\n",
    "        self.final_gap = [0.] * self.num_nodes\n",
    "        \n",
    "        t = 0\n",
    "        \n",
    "        for date in z1.keys():\n",
    "            \n",
    "            for i,loader in enumerate(self.dataloader):\n",
    "                truez, predz = ModelPrediction(self.models[i], date, loader,lookahead)\n",
    "                path = path_figure_date+\"/Model_\"+str(i)+\"/\"\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "                self.plotPrediction(truez, predz,date,path_to_save=path)\n",
    "            \n",
    "            for (couple1, couple2, couple4,couple5) in zip(z1[date],z2[date], z4[date], z5[date]):\n",
    "                datazones = [self.__nodeInit(*couple1), \n",
    "                             self.__nodeInit(*couple2),\n",
    "                             self.__nodeInit(*couple4),\n",
    "                             self.__nodeInit(*couple5)]\n",
    "                \n",
    "\n",
    "                for i in range(self.num_nodes):\n",
    "                    self.initModelWeight(self.models[i])\n",
    "                    self.models[i].train()\n",
    "\n",
    "                    def closure():\n",
    "                        self.optimizers[i].zero_grad(set_to_none=True)\n",
    "                        x, y = iter(datazones[i]).next()\n",
    "                        output = self.models[i](x)\n",
    "                        loss = self.loss(output,y)\n",
    "                        loss.backward()\n",
    "\n",
    "                    self.optimizers[i].initValue(closure)\n",
    "                \n",
    "                for l in range(L):\n",
    "                    #print(\"--------------------------\")\n",
    "                    for i in range(self.num_nodes):\n",
    "                        self.optimizers[i].neighborsAverage(self.optimizers)\n",
    "                    for i in range(self.num_nodes):\n",
    "                        \n",
    "                        def closure():\n",
    "                            self.optimizers[i].zero_grad(set_to_none=True)\n",
    "                            x, y = iter(datazones[i]).next()\n",
    "                            output = self.models[i](x)\n",
    "                            loss = self.loss(output, y)\n",
    "                            loss.backward()\n",
    "                            \n",
    "                        self.optimizers[i].step(l, closure)\n",
    "\n",
    "                self.gaps_off = [0.] * self.num_nodes\n",
    "                for i in range(self.num_nodes):\n",
    "                    with torch.no_grad():\n",
    "                        self.models[i].eval()\n",
    "                        x, y = iter(datazones[i]).next()\n",
    "                        outputs = self.models[i](x)\n",
    "                        curr_loss = self.loss(outputs, y)\n",
    "                    self.final_gap[i] += self.optimizers[i].init_gap\n",
    "                    self.final_gap[i] /= (t + 1)\n",
    "                    self.gaps_off[i] = self.optimizers[i].init_gap\n",
    "                    self.losses[i] = curr_loss.detach().numpy()\n",
    "\n",
    "                loss = np.mean(self.losses)\n",
    "                gap = np.max(self.final_gap)  #.detach().numpy(\n",
    "                local_gap = np.max(self.gaps_off)\n",
    "                if t % 1 == 0:\n",
    "                    print(\"t_{} : loss : {:.5f} gap : {} local_gap {}\".format(\n",
    "                        t, loss, gap, local_gap))\n",
    "                    \n",
    "                self.obj_values[t, :] = [t, loss, gap, local_gap]\n",
    "                    \n",
    "                t+=1\n",
    "            \n",
    "\n",
    "        return self.obj_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T21:08:28.030694Z",
     "start_time": "2021-08-02T21:08:28.010457Z"
    },
    "code_folding": [
     0,
     16,
     32
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers=1):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hid_dim, n_layers, batch_first=False, bias=False)\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        outputs, self.hidden = self.lstm(seq.view(seq.shape[1],seq.shape[0],self.input_dim))\n",
    "        return outputs, self.hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.n_layers, batch_size, self.hid_dim),\n",
    "                torch.zeros(self.n_layers, batch_size, self.hid_dim))\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, output_dim, n_layers=1):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hid_dim, n_layers, batch_first = False, bias=False)\n",
    "        self.fc = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, encoder_hidden):\n",
    "        lstm_out, self.hidden = self.lstm(x.unsqueeze(0), encoder_hidden)\n",
    "        output = self.fc(lstm_out.squeeze(0))\n",
    "        return output, self.hidden\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, output_dim, ts_out, forcing = 1):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.ts_out = ts_out\n",
    "        self.encoder = Encoder(input_dim, hid_dim)\n",
    "        self.decoder = Decoder(input_dim,hid_dim,output_dim)\n",
    "        self.forcing = forcing\n",
    "\n",
    "        \n",
    "    def forward(self, source, y=None):\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = self.ts_out\n",
    "        outputs = torch.zeros(target_len, batch_size,self.output_dim)\n",
    "        encoder_hidden = self.encoder.init_hidden(batch_size)\n",
    "        encoder_output, encoder_hidden = self.encoder(source)\n",
    "        \n",
    "        decoder_input = source[:,-1,:]\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output\n",
    "            if (y is not None) and (t>1) and (torch.rand(1) < self.forcing):\n",
    "                decoder_input = y[:,t].unsqueeze(1)\n",
    "            else :\n",
    "                decoder_input = decoder_output\n",
    "        #print(outputs.shape)\n",
    "        return outputs.squeeze().permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T21:08:29.240611Z",
     "start_time": "2021-08-02T21:08:29.233289Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, time_step_in, time_step_out):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.time_step_in = time_step_in\n",
    "        self.time_step_out = time_step_out\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        self.encoder = nn.LSTM(self.input_size, self.output_size,\n",
    "                               num_layers=self.num_layers, batch_first=True, bias=True)\n",
    "        \n",
    "        self.batch = nn.BatchNorm1d(self.output_size)\n",
    "        self.linear2 = nn.Linear(self.num_layers*self.output_size, self.time_step_out)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out_en, (h_en,_) = self.encoder(x)\n",
    "        h_en = h_en.view(-1, self.num_layers*self.output_size)\n",
    "        h_en = self.batch(h_en)\n",
    "        out = nn.LeakyReLU()(h_en)\n",
    "        #out = torch.sigmoid(self.linear(out))\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:19:31.574686Z",
     "start_time": "2021-08-02T22:19:31.563184Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self,output_chan, output_dim,input_dim,kernel_size):\n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        self.output_chan = output_chan\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=output_chan, kernel_size=kernel_size, stride=1, \n",
    "                                padding=int(np.floor(kernel_size/2)))\n",
    "        \n",
    "        \n",
    "        self.maxpool = nn.MaxPool1d(3)\n",
    "        self.batchnorm = nn.BatchNorm1d(output_chan)\n",
    "        self.fc1 = nn.Linear(output_chan*int((input_dim/3)), output_dim, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs = x.unsqueeze(1).squeeze(-1)\n",
    "        #print(inputs.shape)\n",
    "        out = self.conv1d(inputs)\n",
    "        out = self.batchnorm(out)\n",
    "        #print(out.shape)\n",
    "        out = self.maxpool(out)\n",
    "        #print(out.shape)\n",
    "        out = nn.LeakyReLU()(out)\n",
    "        \n",
    "        out = out.view(-1, out.shape[1]*out.shape[2])\n",
    "        \n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T18:07:51.075669Z",
     "start_time": "2021-08-02T18:07:51.068916Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, nb_units, input_dim, output_dim):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        self.nb_units = nb_units\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.nb_units2 = 16\n",
    "    \n",
    "        self.fc1 = nn.Linear(input_dim, nb_units)\n",
    "        #self.fc2 = nn.Linear(nb_units, self.nb_units2)\n",
    "        self.fc3 = nn.Linear(nb_units, output_dim)\n",
    "        \n",
    "        self.batch1 = nn.BatchNorm1d(nb_units)\n",
    "        #self.batch2 = nn.BatchNorm1d(self.nb_units2)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        inputs = x.squeeze(-1)\n",
    "        out = self.fc1(inputs)\n",
    "        out = self.batch1(out)\n",
    "        out = nn.ReLU()(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T18:07:53.436744Z",
     "start_time": "2021-08-02T18:07:53.432263Z"
    }
   },
   "outputs": [],
   "source": [
    "complet_graph, complet = Graphs.completegraph(4)\n",
    "cycle_graph, cycle = Graphs.cycle_graph(4)\n",
    "grid_graph, grid = Graphs.gridgraph(2,2)\n",
    "grid_graph_line, line = Graphs.gridgraph(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:13:44.745488Z",
     "start_time": "2021-08-02T22:13:44.741851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:58:12.918519Z",
     "start_time": "2021-08-02T22:58:12.913989Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.SmoothL1Loss()\n",
    "num_iters_base = nb_iterations\n",
    "eta_coef_DMFW = 1\n",
    "eta_exp_DMFW = 0.75\n",
    "rho_coef_DMFW = 4e-0\n",
    "rho_exp_DMFW = 1/2\n",
    "reg_coef_DMFW = 10\n",
    "L_DMFW = 50 #or nb_iteration to get a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:55:36.106138Z",
     "start_time": "2021-08-02T22:55:36.096966Z"
    }
   },
   "outputs": [],
   "source": [
    "def ModelPrediction(model_to_test, date ,loader, lookahead):\n",
    "    prediction = []\n",
    "    true = []\n",
    "    for val, valpred in loader[date]:\n",
    "        model_to_test.eval()\n",
    "        pred = model_to_test(val)\n",
    "        #print(pred.shape)\n",
    "        prediction.append(pred.detach().numpy())\n",
    "        true.append(valpred.detach().numpy())\n",
    "    pred_array = np.asarray(prediction)\n",
    "    true_array = np.asarray(true)\n",
    "    #print(pred_array.shape)\n",
    "    pred_shape = pred_array.shape\n",
    "    #print(pred_shape)\n",
    "    flattenTrue = true_array.reshape(pred_shape[0]*pred_shape[1], lookahead)[::lookahead].flatten()\n",
    "    flattenPred = pred_array.reshape(pred_shape[0]*pred_shape[1], lookahead)[::lookahead].flatten()\n",
    "    return flattenTrue, flattenPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:58:13.979075Z",
     "start_time": "2021-08-02T22:58:13.975056Z"
    }
   },
   "outputs": [],
   "source": [
    "trainXMFW = Trainer(cycle_graph,trainloader,CNN1D, (8,lookahead,lookback,5), loss_fn,num_iters_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:33:51.780802Z",
     "start_time": "2021-08-02T22:58:19.025559Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "values_dmfw = trainXMFW.train(DMFW, L_DMFW, eta_coef_DMFW, eta_exp_DMFW, reg_coef_DMFW,1,\n",
    "                            path_figure_date=\"../OnlineModels/CNN1D/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:35:11.604482Z",
     "start_time": "2021-08-02T23:35:11.397212Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"{}\".format(cycle))\n",
    "plt.plot(values_dmfw[:,0][:-1],values_dmfw[:,2][:-1], label='DMFW', marker='^', markersize=4,\n",
    "         markevery=[i for i in range(len(values_dmfw[:,0][1:])) if i%10==0])\n",
    "plt.axhline(y=0, color='grey', linestyle='--')\n",
    "plt.legend(loc='upper right')\n",
    "#plt.ylim((1e-4, 1e0))\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"#Iterations\",fontsize=15)\n",
    "plt.ylabel(\"Gap\",fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:35:14.305891Z",
     "start_time": "2021-08-02T23:35:14.301650Z"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(values_dmfw[:,1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:35:15.966335Z",
     "start_time": "2021-08-02T23:35:15.962810Z"
    }
   },
   "outputs": [],
   "source": [
    "onlineloss = np.cumsum(values_dmfw[:,1][:-1])\n",
    "arangement = np.arange(1,len(onlineloss)+1)\n",
    "onlineloss = onlineloss/arangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:35:16.602812Z",
     "start_time": "2021-08-02T23:35:16.420776Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle(\"{}\".format(cycle))\n",
    "plt.plot(values_dmfw[:,0][:-1],values_dmfw[:,1][:-1], label='Step Loss', marker='^', markersize=4,\n",
    "         markevery=[i for i in range(len(values_dmfw[:,0][1:])) if i%10==0])\n",
    "plt.plot(values_dmfw[:,0][:-1],onlineloss, label='Online Loss', marker='^', markersize=4,\n",
    "         markevery=[i for i in range(len(values_dmfw[:,0][1:])) if i%10==0])\n",
    "plt.axhline(y=0, color='grey', linestyle='--')\n",
    "plt.legend(loc='upper right')\n",
    "#plt.ylim((1e-3, 1e2))\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"#Iterations\",fontsize=15)\n",
    "plt.ylabel(\"Loss\",fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-31T16:55:01.900832Z",
     "start_time": "2021-07-31T16:55:01.896182Z"
    }
   },
   "outputs": [],
   "source": [
    "#torch.save(trainXMFW.models[3].state_dict(),\"./OnlineModel/CNN1D/FINAL43_BestFinal/model3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:35:25.246902Z",
     "start_time": "2021-08-02T23:35:25.236722Z"
    }
   },
   "outputs": [],
   "source": [
    "model_trained = trainXMFW.models[0]\n",
    "true, pred = ModelPrediction(model_trained,\"2019-05-16\", loaderZ1test, lookahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:35:25.635674Z",
     "start_time": "2021-08-02T23:35:25.557961Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(true) \n",
    "plt.plot(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:55:43.081243Z",
     "start_time": "2021-08-02T22:55:43.070309Z"
    }
   },
   "outputs": [],
   "source": [
    "def PredictionSlidingWindow(model, dates, date_begin, date_end, loader, lookahead, scaler, zone_name, ft_idx):\n",
    "    predictTrue = []\n",
    "    predictPred = []\n",
    "    valuePD = pd.DataFrame()\n",
    "    idx_begin = dates.index(date_begin)\n",
    "    idx_end = dates.index(date_end)\n",
    "    for date in dates[idx_begin:idx_end]:\n",
    "        true, pred = ModelPrediction(model, date, loader, lookahead)\n",
    "        maxi = scalers[zone_name].data_max_[ft_idx]\n",
    "        mini = scalers[zone_name].data_min_[ft_idx]\n",
    "        true = true.reshape(-1,1)*(maxi-mini) + mini\n",
    "        pred = pred.reshape(-1,1)*(maxi-mini) + mini\n",
    "        array = np.concatenate([true, pred],axis=1)\n",
    "        result = pd.DataFrame({\"Truth\": true.squeeze(),\n",
    "                              \"Prediction\" : pred.squeeze()})\n",
    "                              #\"Date\": pd.date_range(start=date,periods=true.shape[0], freq=\"5min\")})\n",
    "        valuePD = pd.concat((valuePD, result)).reset_index(drop=True)\n",
    "    return valuePD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T22:55:44.082737Z",
     "start_time": "2021-08-02T22:55:44.078206Z"
    }
   },
   "outputs": [],
   "source": [
    "floor_dict[\"Floor7Z1\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:40:18.810217Z",
     "start_time": "2021-08-02T23:40:18.702582Z"
    }
   },
   "outputs": [],
   "source": [
    "ft_idx = 3\n",
    "indexroom = 3\n",
    "mymodel = trainXMFW.models[indexroom]\n",
    "testloader = [loaderZ1test, loaderZ2test, loaderZ4test, loaderZ5test]\n",
    "may = [date for date in test_date if '2019-05' in date]\n",
    "myfloor = [\"Floor7Z1\",\"Floor7Z2\",\"Floor7Z4\",\"Floor7Z5\"]\n",
    "resultpd = PredictionSlidingWindow(mymodel,test_date,may[0], may[-1],testloader[indexroom], lookahead, scalers,\n",
    "                                 myfloor[indexroom], ft_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-02T23:40:19.027985Z",
     "start_time": "2021-08-02T23:40:18.944925Z"
    }
   },
   "outputs": [],
   "source": [
    "resultpd[[\"Truth\",\"Prediction\"]].plot(figsize=(15,5),title=\"Model_\"+str(indexroom))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-24T09:40:30.932087Z",
     "start_time": "2021-07-24T09:40:30.927551Z"
    }
   },
   "source": [
    "floor_dict[\"Floor7Z1\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-24T09:40:08.966140Z",
     "start_time": "2021-07-24T09:40:08.957135Z"
    }
   },
   "outputs": [],
   "source": [
    "scalers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
